{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\machine_learning\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\anaconda\\envs\\machine_learning\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\anaconda\\envs\\machine_learning\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import umap # dimensionality reduction\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import hdbscan\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>contents</th>\n",
       "      <th>agency_id</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>سمو ولي العهد يغادر مصر بعد المشاركة في مؤتمري...</td>\n",
       "      <td>أخبار محلية</td>\n",
       "      <td>\\n                                    بحفظ الل...</td>\n",
       "      <td>الوطن</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>الأمانة العامة للمجلس الأعلى للتخطيط والتنمية ...</td>\n",
       "      <td>أخبار إقتصادية</td>\n",
       "      <td>\\r\\n                                          ...</td>\n",
       "      <td>الأمانة العامة للمجلس الأعلى للتخطيط والتنمية</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\r\\n                                    \"التأم...</td>\n",
       "      <td>أخبار محلية</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n \\n\\n\\n</td>\n",
       "      <td>المؤسسة العامة للتأمينات الاجتماعية</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>تمديد طرح الممارسة رقم  ص و م م 4/ 2022/2023م ...</td>\n",
       "      <td>أخبار إقتصادية</td>\n",
       "      <td>تمديد طرح الممارسة رقم  ص و م م 4/ 2022/2023م ...</td>\n",
       "      <td>الصندوق الوطني</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>بسم اللّه الرحمن الرحيم</td>\n",
       "      <td>أخبار محلية</td>\n",
       "      <td></td>\n",
       "      <td>جهاز متابعة الأداء الحكومي</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title     category_id  \\\n",
       "0  سمو ولي العهد يغادر مصر بعد المشاركة في مؤتمري...     أخبار محلية   \n",
       "1  الأمانة العامة للمجلس الأعلى للتخطيط والتنمية ...  أخبار إقتصادية   \n",
       "2  \\r\\n                                    \"التأم...     أخبار محلية   \n",
       "3  تمديد طرح الممارسة رقم  ص و م م 4/ 2022/2023م ...  أخبار إقتصادية   \n",
       "4                            بسم اللّه الرحمن الرحيم     أخبار محلية   \n",
       "\n",
       "                                            contents  \\\n",
       "0  \\n                                    بحفظ الل...   \n",
       "1  \\r\\n                                          ...   \n",
       "2                                \\n\\n\\n\\n\\n\\n \\n\\n\\n   \n",
       "3  تمديد طرح الممارسة رقم  ص و م م 4/ 2022/2023م ...   \n",
       "4                                                      \n",
       "\n",
       "                                       agency_id  tweet_text  \n",
       "0                                          الوطن         NaN  \n",
       "1  الأمانة العامة للمجلس الأعلى للتخطيط والتنمية         NaN  \n",
       "2            المؤسسة العامة للتأمينات الاجتماعية         NaN  \n",
       "3                                 الصندوق الوطني         NaN  \n",
       "4                     جهاز متابعة الأداء الحكومي         NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.read_csv(\"C:/Users/rbw19/Downloads/dataset.csv\",on_bad_lines='skip')\n",
    "dataset=dataset[[\"title\",'category_id','contents',\"agency_id\",'tweet_text']]\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp='[\\u0627-\\u064aA-Za-z]+' #re to get arabic and english , i had eliminated numbers and special characters because they dont have much influence \n",
    "dataset[\"title\"]=dataset[\"title\"].apply(lambda data : \" \".join(re.findall(exp,data)))\n",
    "dataset['category_id']=dataset[\"category_id\"].apply(lambda data: \" \".join(re.findall(exp,data)) )\n",
    "dataset[\"contents\"]=dataset['contents'].apply(lambda data : \" \".join(re.findall(exp,data)))\n",
    "dataset[\"agency_id\"]=dataset[\"agency_id\"].apply(lambda data : \" \".join(re.findall(exp,data)))\n",
    "dataset[\"tweet_text\"]=dataset[\"tweet_text\"].apply(lambda data: \" \".join(re.findall(exp,data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    # text = text.lower()\n",
    "    # text = text.replace('x', '')\n",
    "    return text\n",
    "\n",
    "dataset[\"title\"]=dataset[\"title\"].apply(lambda data:cleanText(data))\n",
    "dataset[\"contents\"]=dataset[\"contents\"].apply(lambda data:cleanText(data))\n",
    "dataset[\"category_id\"]=dataset[\"category_id\"].apply(lambda data:cleanText(data))\n",
    "dataset[\"agency_id\"]=dataset[\"agency_id\"].apply(lambda data:cleanText(data))\n",
    "dataset[\"tweet_text\"]=dataset[\"tweet_text\"].apply(lambda data:cleanText(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataset[\"title\"]+dataset[\"contents\"]+dataset[\"category_id\"]+dataset[\"agency_id\"]+dataset[\"tweet_text\"]\n",
    "data=data.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" # this is already a pretrained model ,so need of saving this model with our data\n",
    "model = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_gen = model(data)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_gen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings = (umap.UMAP(n_neighbors=10, n_components=20, metric='cosine', \n",
    "                                random_state=24).fit_transform(embedding_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(umap_embeddings),'\\n',umap_embeddings.shape,'\\n',umap_embeddings)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = hdbscan.HDBSCAN(min_cluster_size = 3,\n",
    "                               metric='euclidean', \n",
    "                               cluster_selection_method='eom').fit(umap_embeddings)\n",
    "# hdbscan.HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,\n",
    "#                 gen_min_span_tree=False, leaf_size=40,metric='euclidean',\n",
    "#                 min_cluster_size=5, min_samples=None, p=None)\n",
    "print(clusters.labels_,len(clusters.labels_))\n",
    "print(np.unique(clusters.labels_),'\\n',len(np.unique(clusters.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=[]\n",
    "for idx in range(len(clusters.labels_)):\n",
    "    if clusters.labels_[idx]==10: #taking dataponits of 10th cluster\n",
    "        cluster.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_cluster=[]\n",
    "for dp in cluster:\n",
    "    news_cluster.append(GoogleTranslator(source='auto', target='en').translate(data[dp][:200])) #limiting to 200 --> for easy translation\n",
    "news_cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('machine_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9c62d8cf6e30c1639f8e9a0a261a530483935ac00f94e2882c1ade2d532fd4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
